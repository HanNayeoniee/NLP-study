# ğŸ« Week 04-Attention Mechanism

### Seq2seq(encoder-decoder) model
<img src="https://user-images.githubusercontent.com/33839093/110282162-5a2b5f00-8021-11eb-9c4b-d0bd65e7f831.png" width="- 600">

> - ë‹¨ì–´ë³„ë¡œ ë²ˆì—­í•˜ëŠ”ê±´ ë³„ë¡œ ì¢‹ì€ ë°©ë²•ì´ ì•„ë‹˜: ì˜ˆ) "I love you"ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ë©´ "ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´"ì´ì§€ë§Œ, ì´ë¥¼ ë‹¨ì–´ ê¸°ì¤€ìœ¼ë¡œ ë²ˆì—­í•˜ë©´ 'I-ë‚˜', 'love-ë„ˆë¥¼', 'you-ì‚¬ë‘í•´'ë¡œ ë²ˆì—­ë˜ëŠ” ì˜¤ë¥˜ê°€ ìˆê¸° ë•Œë¬¸
> - encoder : ê° ë‹¨ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë°›ì•„ context vectorë¥¼ ë§Œë“¦
> - decoder : context vectorë¡œë¶€í„° ê¸°ê³„ë²ˆì—­ì„ ì‹œì‘
> - ë¬¸ì œì  : context vectorëŠ” ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ì´ê¸° ë•Œë¬¸ì— ë¬¸ì¥ì´ ê¸¸ì–´ì§€ê±°ë‚˜ context vectorì˜ í¬ê¸°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì…ë ¥ ë‹¨ì–´ì˜ ëª¨ë“  ì˜ë¯¸ë¥¼ í•¨ì¶•í•˜ê¸° ì–´ë ¤ì›€
> - í•´ê²° ë°©ë²• : attention mechanism

### Attention Mechanism
<img src="https://user-images.githubusercontent.com/33839093/110282637-2ac92200-8022-11eb-99df-ed670786e24a.png" widt="600">

> - ê¸°ì¡´ seq2seq ëª¨ë¸ : encoderì—ì„œ ë‚˜ì˜¨ ë§¨ ë§ˆì§€ë§‰ stateë§Œ ì‚¬ìš©í•´ context vecctorë¥¼ êµ¬ì„±í•¨
> - encoderì—ì„œ ë‚˜ì˜¨ stateë¥¼ decoderì—ì„œ dynamicí•˜ê²Œ context vectorë¥¼ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ì!
> - ì¥ì  : context vectorê°€ ê³ ì •ëœ í¬ê¸°ê°€ ì•„ë‹˜, encoderì˜ stateì—ì„œ ì§‘ì¤‘í•´ì•¼í•  ë‹¨ì–´ë“¤ë§Œ ì„ íƒí•´ ì§‘ì¤‘í•˜ëŠ” mechanismì„ ì„¤ê³„í•  ìˆ˜ ìˆìŒ

### Seq2seq with attention mechanism
- step 1
<img src="https://user-images.githubusercontent.com/33839093/110282829-809dca00-8022-11eb-8663-9553adb72128.png" width="600">

> 1) h3 = ì „í†µì ì¸ ëª¨ë¸ì—ì„œì˜ context vector
> 2) RNNì…€ì˜ ëª¨ë“  ê°’ì„ í™œìš©í•´ FC layerë¥¼ ê±°ì¹¨
> 3) s1, s2, s3ëŠ” RNNì…€ì— ìˆë˜ encoderì˜ scoreë¥¼ ì˜ë¯¸í•¨
> 4) Softmaxë¥¼ ì·¨í•˜ë©´ ê°ê°ì€ í™•ë¥ ê°’ì„ ê°€ì§
	IëŠ” 0.9, loveëŠ” 0.0, youëŠ” 0.1ì˜ ê°’ì„ ê°€ì§. ì´ ê°’ì„ attention weightë¼ê³  ë¶€ë¥´ë©°, ê° ë‹¨ì–´ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•  ê²ƒì¸ì§€ ì˜ë¯¸í•¨
> 5) Attention weightì— ë”°ë¼ ì²«ë²ˆì§¸ context vectorë¥¼ ë§Œë“¦

- step 2
<img src="https://user-images.githubusercontent.com/33839093/110283097-dffbda00-8022-11eb-9da8-f78ccfc2d047.png" width="600">

> - Decoderì˜ ê°’ì¸ dh1ì´ FC layerì— ë“¤ì–´ê°
> - Encoder stateê°’ì¸ h1, h2, h3ì€ ë§¤ë²ˆ ì‚¬ìš©ë¨
> - ë°”ë€ FC layer(h1, h2, h3, df1)ì„ í†µí•´ attention weightë¥¼ ë‹¤ì‹œ ê³„ì‚°
> - ë‘ë²ˆì§¸ context vectorì¸ cv2ë¥¼ ê³„ì‚°í•´ ëª¨ë¸ì— ë„£ì–´ì£¼ë©´ 'ë„'ì„ ì¶œë ¥í•¨

- step 3
<img src="https://user-images.githubusercontent.com/33839093/110283282-27826600-8023-11eb-8910-c7f81deb8426.png" width="600">
> - 3ë²ˆì§¸ attention weightì—ì„œëŠ” 'love'ì˜ ê°’ì´ 0.95ë¡œ ë§ì´ ì§‘ì¤‘í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìœ¼ë©° 'ì‚¬ë‘í•´'ë¥¼ ì¶œë ¥í•¨

### ì¤‘ìš”
	- Attention weightë¥¼ í†µí•´ encoderì—ì„œ ë‚˜ì˜¨ stateì¤‘ì— ì–´ë””ì— ì§‘ì¤‘í• ì§€ ê²°ì •í•¨
	- Decodingí•  ë•Œë§ˆë‹¤ context vectorê°€ ë‹¬ë¼ì§
	- Teacher forcing : ì²«ë²ˆì§¸ decoding ê²°ê³¼ê°€ 'ë‚œ'ì´ ì•„ë‹ˆë¼ 'ë„ˆ'ì²˜ëŸ¼ í‹€ë¦° predictionì„ í•˜ëŠ” ê²½ìš°ì—ëŠ” ì •ë‹µì¸ 'ë‚œ'ì„ ë„£ì–´ì„œ í•™ìŠµì‹œí‚´

### References
- ë…¼ë¬¸: Neural machine translation by jointly learning to align and translate
- ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ì²˜ë¦¬ ì…ë¬¸ (https://wikidocs.net/24996)
- ì‹œí€€ìŠ¤ íˆ¬ ì‹œí€€ìŠ¤+ì–´í…ì…˜ ê°•ì˜ (https://www.youtube.com/watch?v=WsQLdu2JMgI)
